{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Keyword Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading ld_resume.py python file on the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ld_resume.py\n",
    "import textract\n",
    "import re\n",
    "import argparse\n",
    "import os\n",
    "import ssl\n",
    "import nltk\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import Counter\n",
    "import json\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def PrintHelp():\n",
    "    \"\"\"\n",
    "    Function to Print out the help section of the code\n",
    "    \"\"\"\n",
    "    usage_text = \"\"\"Usage: python ld_resume.py [ARGS] \n",
    "                  Required Arguments: \n",
    "                  --dir </dir/to/folder/>              Path to folder containing resumes \n",
    "                  --result-file <output json file>     Path to Output Json File \n",
    "\n",
    "                  Optional Arguments: \n",
    "                  --min-df <min_df value>              Min DF value for the CountVectorizer\n",
    "                  --max-df <max_df value>              Max DF value for the CountVectorizer\n",
    "                  --stop-words <stop words>            Custom Stop Words if any \n",
    "                  --num-common-words-ignore <Number>   Number of common words to ignore\n",
    "                  --num-keywords <Number>              Number of keywords to extract to JSON. default = 5 \n",
    "\n",
    "                  Example Usage : python ld_resume.py --dir /path/to/folder_containing_resumes/ --result-file /path/output/result.json\"\"\"\n",
    "\n",
    "    print(usage_text)\n",
    "\n",
    "\n",
    "def create_json_output_file(json_object, file_path):\n",
    "    \"\"\"\n",
    "    Function to serialize an object in a JSON formatted stream and write to a .json file\n",
    "    Function creates the base directory folders of the final output file, if the directories do not exist.\n",
    "    If the output file already exists, a new file will be created in its place.\n",
    "    The indent level of the JSON file is 2, to pretty print.\n",
    "    input:\n",
    "        json_object - Object to serialize in JSON format\n",
    "        file_path - location of the JSON file\n",
    "    return value : No return value\n",
    "    \"\"\"\n",
    "    dir_name = os.path.dirname(file_path)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        json.dump(json_object, outfile, sort_keys=True, indent=2)\n",
    "\n",
    "    print(json.dumps(json_object, indent=True, sort_keys=True))\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "def extract_topn_keywords(feature_names, sorted_tf_idf_vector, topn=10):\n",
    "    \"\"\"\n",
    "    Function to get the top N feature names from the tf-idf vector\n",
    "    The function filters out unigrams/bigrams, based on if a part of was already included in the list.\n",
    "    :arg\n",
    "        feature_name - list of feature names extracted from the CountVector\n",
    "        sorted_tf_idf_vector - Sorted TF-IDF vector created from the TFIDFTransformer\n",
    "        topn - Number of top N feature names to extract from the vector\n",
    "    :return\n",
    "        dict object containing top N feature names and their scores\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in (range(len(sorted_tf_idf_vector))):\n",
    "        if str(feature_names[i]) not in result:\n",
    "            # print(\"Checking \" + feature_names[sorted_items[i][0]])\n",
    "            split = str(feature_names[sorted_tf_idf_vector[i][0]]).split(\" \")\n",
    "            if len(split) == 1:\n",
    "                found = 0\n",
    "                for entry in result:\n",
    "                    match = SequenceMatcher(None, split[0], entry).find_longest_match(0, len(split[0]), 0, len(entry))\n",
    "                    if match.size > 5:\n",
    "                        found = 1\n",
    "                if found == 0:\n",
    "                    result[feature_names[sorted_tf_idf_vector[i][0]]] = sorted_tf_idf_vector[i][1]\n",
    "\n",
    "            else:\n",
    "                for word in split:\n",
    "                    if word in result:\n",
    "                        result.pop(word, None)\n",
    "                        result[feature_names[sorted_tf_idf_vector[i][0]]] = sorted_tf_idf_vector[i][1]\n",
    "                    else:\n",
    "                        result[feature_names[sorted_tf_idf_vector[i][0]]] = sorted_tf_idf_vector[i][1]\n",
    "\n",
    "        if len(result) == topn:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_resume_filepaths(input_folder):\n",
    "    \"\"\"\n",
    "    Function to get list of file paths to resumes in pdf format.\n",
    "        :arg\n",
    "            Folder to path containing files\n",
    "        :return\n",
    "            List of file paths\n",
    "    \"\"\"\n",
    "    list_resumes = []\n",
    "    for dirpath, _, filenames in os.walk(input_folder):\n",
    "        for f in filenames:\n",
    "            if \"pdf\" in f:\n",
    "                list_resumes.append(os.path.abspath(os.path.join(dirpath, f)))\n",
    "\n",
    "    return list_resumes\n",
    "\n",
    "\n",
    "def MostCommonWords(resume_store, num=20):\n",
    "    \"\"\"\n",
    "    Function to return top N common words in the whole text except the stop words from the nltk library\n",
    "    This function is used for analyzing if any keywords need to be custom added to the list of stopwords\n",
    "        :arg\n",
    "            resume_store - data frame with column \"ResumeText\" containing resumes and\n",
    "            num - Number of most common words from the combined corpus of entries under \"ResumeText\"\n",
    "        :return\n",
    "            List of top N common words\n",
    "    \"\"\"\n",
    "    num = 40\n",
    "    words = []\n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    for resume_words in resume_store[\"ResumeText\"]:\n",
    "        text = resume_words\n",
    "        # Remove anything which is not except hypen\n",
    "        text = re.sub('[^a-zA-Z0-9|^\\-]', ' ', text)\n",
    "\n",
    "        # Remove words with digits\n",
    "        text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "\n",
    "        # Remove empty hyphens\n",
    "        text = re.sub(' - ', ' ', text)\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word).lower() for word in text if not word in nltk_stop_words]\n",
    "        words.extend(text)\n",
    "\n",
    "    freq = pd.Series(words).value_counts()[:num]\n",
    "\n",
    "    freq.plot(kind=\"barh\")\n",
    "    counter = Counter(words)\n",
    "    common_n = [str(tup[0]).lower() for tup in counter.most_common(num)]\n",
    "    return common_n\n",
    "\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"\n",
    "    Function to sort and return vector of coo matrix generated from tf-idf transformer\n",
    "    :arg\n",
    "        coo_matrix\n",
    "    :return\n",
    "        sorted list of tuples of the sparse coo matrix\n",
    "    \"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "\n",
    "def resume_analyzer(input_folder, output_file, min_df, max_df, stopwords_user=[], num_common_words_in_stopwords=0,\n",
    "                    num_keywords=5):\n",
    "    \"\"\"\n",
    "        Function to analyze resumes in the input folder and extract N keywords into a JSON file.\n",
    "\n",
    "    :param input_folder: folder containing resumes\n",
    "    :param output_file: path to result JSON file\n",
    "    :param min_df: min_df for the CountVectorizer\n",
    "    :param max_df: max_df for the CountVectorizer\n",
    "    :param stopwords_user: Custom Stop words if any provided by the user\n",
    "    :param num_common_words_in_stopwords: Number of most common words to include in the stop words\n",
    "    :param num_keywords: Number of keywords to extract in the JSON\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    list_resumes = get_resume_filepaths(input_folder)\n",
    "    if len(list_resumes) == 0:\n",
    "        print(\"No resumes found in input folder \" + input_folder + \"\\n Exiting program\")\n",
    "        PrintHelp()\n",
    "        return\n",
    "\n",
    "    resume_data_store = pd.DataFrame(columns=[\"PdfFile\", \"ResumeText\"])\n",
    "\n",
    "    # Extract Text from the pdfs and store in a Pandas Dataframe - resume_data_store\n",
    "    for resume_path in list_resumes:\n",
    "        filename_w_ext = os.path.basename(resume_path)\n",
    "        resume_extract_text = textract.process(resume_path, method='pdfminer', encoding='ascii')\n",
    "        df = pd.DataFrame([[filename_w_ext, str(resume_extract_text.decode(\"ASCII\"))]],\n",
    "                          columns=[\"PdfFile\", \"ResumeText\"])\n",
    "        resume_data_store = resume_data_store.append(df)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Creating a list of stop words and adding custom stopwords obtained by counting the number of common words in the entire resume text corpus\n",
    "    if (num_common_words_in_stopwords > 0):\n",
    "        common_n = MostCommonWords(resume_data_store, num_common_words_in_stopwords)\n",
    "        stop_words = stop_words.union(common_n)\n",
    "\n",
    "    # Custom stop words created by analyzing results from various runs.\n",
    "    new_words_orig = [\"deep\", \"india\", \"http\", \"software\", \"greater\", \"new\", \"york\", \"city\", \"year\", \"month\", \"research\",\n",
    "                 \"assistant\", \"things\",\n",
    "                 \"college\", \"maryland\", \"park\", \"college park\", \"computer\", \"science\", \"area\", #\"intern\", \"data science\",\n",
    "                 \"intern\", \"data\", \"science\",\n",
    "                 \"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\",\n",
    "                 \"november\", \"december\"]\n",
    "\n",
    "    stop_words = stop_words.union(new_words_orig)\n",
    "\n",
    "    if (len(stopwords_user) > 0):\n",
    "        stop_words = stop_words.union(stopwords_user)\n",
    "\n",
    "    resume_text_collection = []\n",
    "\n",
    "    for resume_words in resume_data_store[\"ResumeText\"]:\n",
    "        text = resume_words\n",
    "\n",
    "        # Remove punctuation marks except hyphen\n",
    "        text = re.sub('[^a-zA-Z0-9|^\\-]', ' ', text)\n",
    "\n",
    "        # Remove words containing numbers\n",
    "        text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in stop_words]\n",
    "        text = \" \".join(text)\n",
    "        resume_text_collection.append(text)\n",
    "\n",
    "    # token_pattern - r\"(?u)\\b\\w[\\w-]*\\w\\b\" to respect hyphenated words\n",
    "    cv = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=stop_words, ngram_range=(1, 2), analyzer='word',\n",
    "                         token_pattern=r\"(?u)\\b\\w[\\w-]*\\w\\b\")\n",
    "    X = cv.fit_transform(resume_text_collection)\n",
    "\n",
    "    feature_names = cv.get_feature_names()\n",
    "\n",
    "    result_json = {}\n",
    "    list_entries = []\n",
    "    for i in range(len(resume_data_store)):\n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfidf_transformer.fit(X)\n",
    "        tf_idf_vector = tfidf_transformer.transform(cv.transform([resume_text_collection[i]]))\n",
    "        sorted_tf_idf_vector = sort_coo(tf_idf_vector.tocoo())\n",
    "        # extract only the top n; n here is 10\n",
    "        keywords = extract_topn_keywords(feature_names, sorted_tf_idf_vector, num_keywords)\n",
    "\n",
    "        key_words_resume = []\n",
    "        resume = {}\n",
    "        for k in keywords:\n",
    "            key_words_resume.append(str(k).title())\n",
    "\n",
    "        resume[\"FileName\"] = resume_data_store.iloc[i, 0]\n",
    "        resume[\"Keywords\"] = key_words_resume\n",
    "\n",
    "        list_entries.append(resume)\n",
    "\n",
    "    result_json[\"Resumes\"] = list_entries\n",
    "\n",
    "    create_json_output_file(result_json, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parsing required and optional arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dir', dest='dir', required=False, help='<Required> path to folder containing resumes')\n",
    "    parser.add_argument('--result-file', dest='result_file', required=False,\n",
    "                        help='<Required> path to the result json file')\n",
    "    parser.add_argument('--min-df', dest='cv_min_df', required=False, default=0.2, type=int,\n",
    "                        help='<Optional> Minimum Document Frequency (min_df) value for the CountVectorizer')\n",
    "    parser.add_argument('--max-df', dest='cv_max_df', required=False, default=0.8, type=int,\n",
    "                        help='<Optional> Maximum Document Frequency (min_df) value for the CountVectorizer')\n",
    "    parser.add_argument('--stop-words', dest='stop_words', required=False, default=[], nargs='+',\n",
    "                        help='<Optional> Custom Stopwords to include in the analyses if any')\n",
    "    parser.add_argument('--num-common-words-ignore', dest='num_common_words_in_stopwords', required=False, default=0,\n",
    "                        type=int, help='<Optional> Number of most common words to include in the stop words')\n",
    "    parser.add_argument('--num-keywords', dest='num_keywords', required=False, default=5, type=int,\n",
    "                        help='<Optional> Number of keywords to extract from files')\n",
    "\n",
    "    parser.add_argument(\"--print-help\", dest='print_help', action='store_true')\n",
    "\n",
    "    parsed_args = parser.parse_args();\n",
    "\n",
    "    if parsed_args.print_help:\n",
    "        PrintHelp()\n",
    "        exit()\n",
    "\n",
    "    folder = parsed_args.dir\n",
    "    result_json = parsed_args.result_file\n",
    "\n",
    "    cv_min_df = parsed_args.cv_min_df\n",
    "    cv_max_df = parsed_args.cv_max_df\n",
    "    stopwords_user = parsed_args.stop_words\n",
    "    num_common_words_in_stopwords = parsed_args.num_common_words_in_stopwords\n",
    "    num_keywords = parsed_args.num_keywords\n",
    "\n",
    "    if (str(folder) == \"\"):\n",
    "        print(\"Usage : --dir argument empty\")\n",
    "        PrintHelp()\n",
    "\n",
    "    if (str(result_json) == \"\"):\n",
    "        print(\"Usage : --result-file argument empty\")\n",
    "        PrintHelp()\n",
    "\n",
    "    resume_analyzer(folder, result_json, cv_min_df, cv_max_df, stopwords_user, num_common_words_in_stopwords,\n",
    "                    num_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Pramod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"Resumes\": [\n",
      "  {\n",
      "   \"FileName\": \"Anna Prokofieva.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Google\",\n",
      "    \"Feature\",\n",
      "    \"Experiment\",\n",
      "    \"Columbia University\",\n",
      "    \"Classification\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Pranay Anchuri.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Institute\",\n",
      "    \"Ibm\",\n",
      "    \"Python\",\n",
      "    \"Programs\",\n",
      "    \"Pattern\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Gokul Alex.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Digital\",\n",
      "    \"Blockchain\",\n",
      "    \"Platform\",\n",
      "    \"Innovation\",\n",
      "    \"Technology\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Chris Gonsalves.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Financial\",\n",
      "    \"Community\",\n",
      "    \"Analyst\",\n",
      "    \"Business Development\",\n",
      "    \"Group\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Raul Popa.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Product Management\",\n",
      "    \"Best\",\n",
      "    \"Startup\",\n",
      "    \"Multiple\",\n",
      "    \"Machine Learning\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Danning Sui.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Mathematical\",\n",
      "    \"Consensys\",\n",
      "    \"Prize\",\n",
      "    \"Degree\",\n",
      "    \"Working\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Tyler Swartz.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Draft\",\n",
      "    \"Team\",\n",
      "    \"Dollar\",\n",
      "    \"Advisory Board\",\n",
      "    \"Million\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Nidhiya V Raj.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Community\",\n",
      "    \"Technology\",\n",
      "    \"Project\",\n",
      "    \"Young\",\n",
      "    \"Partnership\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Edward Luo.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Python\",\n",
      "    \"Machine Learning\",\n",
      "    \"Finance\",\n",
      "    \"Sql\",\n",
      "    \"Ibm\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"FileName\": \"Timothy Yu.pdf\",\n",
      "   \"Keywords\": [\n",
      "    \"Analysis\",\n",
      "    \"Python\",\n",
      "    \"Fundamental\",\n",
      "    \"Learning\",\n",
      "    \"Technical\"\n",
      "   ]\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%run ld_resume.py --dir /Users/Tanvi/Documents/TanviApps/tz_projects/LiquidityDigital/debug/test/ --result-file /Users/Tanvi/Documents/TanviApps/tz_projects/LiquidityDigital/debug/output/result_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
